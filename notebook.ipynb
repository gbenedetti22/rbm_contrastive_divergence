{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Assignment 3",
   "id": "494cdf51bb4ed15a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Restricted Boltzmann Machine (RBM) + Constructive Divergence\n",
   "id": "733d48d0774d9c36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Restricted Boltzmann Machine (RBM) and Contrastive Divergence  \n",
    "\n",
    "## Description  \n",
    "We begin by defining the steps of the **Contrastive Divergence (CD)** algorithm, used for training an RBM. Below are the detailed steps to implement a generalized RBM.  \n",
    "\n",
    "### Iterations  \n",
    "\n",
    "1. **Initialization**  \n",
    "   Start with an input vector $ v^{(0)} $ (real-world data).  \n",
    "\n",
    "2. **Hidden Layer Calculation**  \n",
    "   $$\n",
    "   P(h_j^{(0)} \\mid v^{(0)}) = \\sigma\\left(\\sum_i v_i^{(0)} w_{ij} + b_j\\right)\n",
    "   $$  \n",
    "\n",
    "3. **Visible Neurons Reconstruction**  \n",
    "   $$\n",
    "   P(v_i^{(1)} \\mid h^{(0)}) = \\sigma\\left(\\sum_j h_j^{(0)} w_{ij} + a_i\\right)\n",
    "   $$  \n",
    "\n",
    "4. **Hidden Neurons Recalculation**  \n",
    "   $$\n",
    "   P(h_j^{(1)} \\mid v^{(1)}) = \\sigma\\left(\\sum_i v_i^{(1)} w_{ij} + b_j\\right)\n",
    "   $$  \n",
    "\n",
    "5. **Weights Update**  \n",
    "   $$\n",
    "   \\Delta w_{ij} = \\eta \\left( v_i^{(0)} h_j^{(0)} - v_i^{(1)} h_j^{(1)} \\right)\n",
    "   $$  \n",
    "\n",
    "### Notes  \n",
    "- $\\sigma$: Sigmoid activation function.  \n",
    "- $\\eta$: Learning rate.  \n",
    "- Steps 2-4 form a **Gibbs sampling step**, used to approximate the gradient during training."
   ],
   "id": "c6e804945d91e7ba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implementation\n",
    "First, we define the sigmoid function:"
   ],
   "id": "f344904a9eaa2981"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ],
   "id": "10cb80aa8d34a08d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Next, we define methods to calculate the probabilities for hidden and visible neurons:",
   "id": "743c5c328047cd32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Calculate hidden neuron probabilities\n",
    "def prop_up(v, W, b):\n",
    "    return sigmoid(np.dot(v, W) + b)\n",
    "\n",
    "# Calculate visible neuron probabilities\n",
    "def prop_down(h, W, a):\n",
    "    return sigmoid(np.dot(h, W.T) + a)"
   ],
   "id": "35f96eebee4108aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We also need a method to sample binary values from a probability distribution:",
   "id": "c1831e1e57645525"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def sample_prob(probs):\n",
    "    return np.random.binomial(1, probs)"
   ],
   "id": "87929a655cd1745c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The Contrastive Divergence algorithm can then be implemented as follows:",
   "id": "6be4afa6fc74792a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def contrastive_divergence(v0, W, a, b, lr, k=1):\n",
    "    # Step 1: v0 (real input data)\n",
    "    # Step 2 (Hidden layer calculation)\n",
    "    h_prob_0 = prop_up(v0, W, b)\n",
    "    h_state = sample_prob(h_prob_0)\n",
    "\n",
    "    for step in range(k):\n",
    "        # Step 3 (Visible Neurons Reconstruction)\n",
    "        v_prob = prop_down(h_state, W, a)\n",
    "        v_state = sample_prob(v_prob)\n",
    "\n",
    "        # Step 4 (Hidden neurons recalc)\n",
    "        h_prob = prop_up(v_state, W, b)\n",
    "        h_state = sample_prob(h_prob)\n",
    "\n",
    "    # Step 5: Update weights and biases\n",
    "    W += lr * (np.outer(v0, h_prob_0) - np.outer(v_state, h_prob))\n",
    "    a += lr * (v0 - v_state)\n",
    "    b += lr * (h_prob_0 - h_prob)\n",
    "\n",
    "    error = np.mean((v0 - v_prob) ** 2)\n",
    "\n",
    "    return W, a, b, error"
   ],
   "id": "898176990a4a4bf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Finally, the training method for our RBM can be written as:",
   "id": "6c0715d15ff6918e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_rbm(x, n_visible, n_hidden, epochs=10, k=1, lr=0.01):\n",
    "    # Weight initialization from Gaussian distribution\n",
    "    W = np.random.normal(0, 0.01, (n_visible, n_hidden))\n",
    "    a = np.zeros(n_visible)  # Visible bias\n",
    "    b = np.zeros(n_hidden)   # Hidden bias\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        error = 0\n",
    "        for v in x:\n",
    "            W, a, b, cd_error = contrastive_divergence(v, W, a, b, lr, k)\n",
    "            error += cd_error\n",
    "\n",
    "        error /= len(x)\n",
    "        print(f\"Epoch {epoch}, Error: {error:.4f}\")\n",
    "\n",
    "    return W, a, b"
   ],
   "id": "2593e27aab2cd69b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Where W, a, and b are the learned weights and biases.",
   "id": "5bb81037aa07c8d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9fdb79741faaaa58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
